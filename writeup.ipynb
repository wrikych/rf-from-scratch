{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A placeholder template for my writeup with the necessary code and details. Fill in and commit gradually. This statement should be the last thing to be edited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal I want to accomplish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Base functions and packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pprint import pprint\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Any credits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logic Behind Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The basic structure \n",
    "    \n",
    "    - General flow\n",
    "    - Base Case - goal = pure node\n",
    "    - Recursive aspect \n",
    "    - Figuring out the best questions to ask - loss functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Information Gain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Tree Bias/Variance tradeoff\n",
    "\n",
    "    - Ensure that the dataset is balanced\n",
    "    - (also other things from DS bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transported\n",
       "True     4378\n",
       "False    4315\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data.Transported.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logic Behind Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensemble learning - basic concepts: benefits, bagging, feature bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bias-Variance trade off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time complexity behind Random Forest vs Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EDA stuff here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_trip(hp, dest):\n",
    "    \n",
    "    trip_val = ''\n",
    "    \n",
    "    if hp == 'Earth' :\n",
    "        if dest == 'TRAPPIST-1e':\n",
    "            trip_val = 'A'\n",
    "        elif dest == '55 Cancri e':\n",
    "            trip_val = 'B'\n",
    "        else:\n",
    "            trip_val = 'C'\n",
    "    elif hp == 'Europa':\n",
    "        if dest == 'TRAPPIST-1e':\n",
    "            trip_val = 'D'\n",
    "        elif dest == '55 Cancri e':\n",
    "            trip_val = 'E'\n",
    "        else:\n",
    "            trip_val = 'F'\n",
    "    elif hp == 'Mars':\n",
    "        if dest == 'TRAPPIST-1e':\n",
    "            trip_val = 'G'\n",
    "        elif dest == '55 Cancri e':\n",
    "            trip_val = 'H'\n",
    "        else:\n",
    "            trip_val = 'I'\n",
    "            \n",
    "    return trip_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(main, sample_percentage):\n",
    "    ## sampling\n",
    "    main = pd.read_csv('train.csv')\n",
    "    data = main.sample(frac=sample_percentage)\n",
    "    \n",
    "    ## rename label\n",
    "    data['label'] = data.Transported\n",
    "    \n",
    "    ## Handle nulls \n",
    "    med_age = data.Age.median()\n",
    "    mode_HP = data.HomePlanet.mode()[0]\n",
    "    mode_CS = data.CryoSleep.mode()[0]\n",
    "    mode_dest = data.Destination.mode()[0]\n",
    "    mode_VIP = data.VIP.mode()[0]\n",
    "    data = data.fillna({'HomePlanet' : mode_HP,\n",
    "                    'CryoSleep' : mode_CS,\n",
    "                    'Destination' : mode_dest,\n",
    "                    'Age' : med_age,\n",
    "                    'VIP' : mode_VIP,\n",
    "                    'RoomService' : 0,\n",
    "                    'FoodCourt' : 0,\n",
    "                    'ShoppingMall' : 0,\n",
    "                    'Spa' : 0,\n",
    "                    'VRDeck' : 0})\n",
    "    \n",
    "    ## Create Exp column \n",
    "    data['Exp'] = data.RoomService + data.FoodCourt + data.ShoppingMall + data.Spa + data.VRDeck\n",
    "    \n",
    "    ## Create trip column\n",
    "    for i, row in data.iterrows():\n",
    "        data.loc[i, 'Trip'] = assign_trip(row['HomePlanet'], row['Destination'])\n",
    "        \n",
    "    ## Fix value spaces \n",
    "    data['Destination'] = data['Destination'].replace('55 Cancri e', '55_Cancri_e')\n",
    "    data['Destination'] = data['Destination'].replace('PSO J318.5-22', 'PSO_J318.5-22 ')\n",
    "    \n",
    "    ## Create two approaches\n",
    "    approach_1 = ['CryoSleep', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Trip', 'label']\n",
    "    approach_2 = ['HomePlanet', 'CryoSleep','Destination', 'Age','VIP',  'Exp', 'label']\n",
    "    \n",
    "    ## Create dataframes\n",
    "    app_1_df = data[approach_1].copy()\n",
    "    app_2_df = data[approach_2].copy()\n",
    "    \n",
    "    return app_1_df, app_2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wriky\\AppData\\Local\\Temp\\ipykernel_19544\\3702823707.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'C' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[i, 'Trip'] = assign_trip(row['HomePlanet'], row['Destination'])\n"
     ]
    }
   ],
   "source": [
    "app1, app2 = data_preprocess(main_data, sample_percentage=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and Information Gain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_entropy(data_below, data_above):\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_entropy =  (p_data_below * calculate_entropy(data_below) \n",
    "                      + p_data_above * calculate_entropy(data_above))\n",
    "    \n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_info_gain(data_unsplit, data_below, data_above):\n",
    "    \n",
    "    unsplit_entropy = calculate_entropy(data_unsplit)\n",
    "    overall = calculate_overall_entropy(data_below, data_above)\n",
    "    \n",
    "    return unsplit_entropy - overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Aspect - Building the Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_purity(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Best Split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scramble(data, max_features):\n",
    "    \n",
    "    feature_ls = list()\n",
    "    num_features = list(data.shape)[1] - 2\n",
    "    \n",
    "    while len(feature_ls) <= max_features:\n",
    "        feature_idx = random.sample(range(num_features), 1)\n",
    "        if feature_idx not in feature_ls:\n",
    "            feature_ls.extend(feature_idx)\n",
    "    \n",
    "    unique = []\n",
    "    \n",
    "    for val in feature_ls:\n",
    "        if val not in unique:\n",
    "            unique.append(val)\n",
    "    \n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_splits(data, max_features):\n",
    "    \n",
    "    \n",
    "    potential_splits = {}\n",
    "    target_features = feature_scramble(data, max_features=max_features)\n",
    "    \n",
    "    for column_index in target_features:          # excluding the last column which is the label\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        \n",
    "        potential_splits[column_index] = unique_values\n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_type_of_feature(df):\n",
    "    \n",
    "    feature_types = []\n",
    "    n_unique_values_treshold = 15\n",
    "    for feature in df.columns:\n",
    "        if feature != \"label\":\n",
    "            unique_values = df[feature].unique()\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "    \n",
    "    return feature_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_split(data, potential_splits): \n",
    "    \n",
    "    best_split_column = None\n",
    "    best_split_value = None\n",
    "    \n",
    "    overall_IG = -999\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            current_IG = calculate_info_gain(data, data_below, data_above)\n",
    "\n",
    "            if current_IG >= overall_IG:\n",
    "                overall_IG = current_IG\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    type_of_feature = FEATURE_TYPES[split_column]\n",
    "    if type_of_feature == \"continuous\":\n",
    "        data_below = data[split_column_values <= split_value]\n",
    "        data_above = data[split_column_values >  split_value]\n",
    "    \n",
    "    # feature is categorical   \n",
    "    else:\n",
    "        data_below = data[split_column_values == split_value]\n",
    "        data_above = data[split_column_values != split_value]\n",
    "    \n",
    "    return data_below, data_above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Tree Structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_algorithm(df, counter, min_samples, max_depth, max_features): \n",
    "    \n",
    "    # data preparations\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS, FEATURE_TYPES\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        FEATURE_TYPES = determine_type_of_feature(df)\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df           \n",
    "    \n",
    "    \n",
    "    # base cases\n",
    "    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        classification = classify_data(data)\n",
    "        \n",
    "        return classification\n",
    "\n",
    "    \n",
    "    # recursive part\n",
    "    else:    \n",
    "        counter += 1\n",
    "\n",
    "        # helper functions \n",
    "        potential_splits = get_potential_splits(data, max_features)\n",
    "        split_column, split_value = determine_best_split(data, potential_splits)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        \n",
    "        # check for empty data\n",
    "        if len(data_below) == 0 or len(data_above) == 0:\n",
    "            classification = classify_data(data)\n",
    "            return classification\n",
    "        \n",
    "        # determine question\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        type_of_feature = FEATURE_TYPES[split_column]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            question = \"{} <= {}\".format(feature_name, split_value)\n",
    "            \n",
    "        # feature is categorical\n",
    "        else:\n",
    "            question = \"{} = {}\".format(feature_name, split_value)\n",
    "        \n",
    "        # instantiate sub-tree\n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        # find answers (recursion)\n",
    "        yes_answer = decision_tree_algorithm(data_below, counter, min_samples, max_depth, max_features)\n",
    "        no_answer = decision_tree_algorithm(data_above, counter, min_samples, max_depth, max_features)\n",
    "        \n",
    "        # If the answers are the same, then there is no point in asking the qestion.\n",
    "        # This could happen when the data is classified even though it is not pure\n",
    "        # yet (min_samples or max_depth base case).\n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Execution - Train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size):\n",
    "    \n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(df))\n",
    "\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bootstrap(data):\n",
    "    bootstrap_indices = list(np.random.choice(range(len(data)), len(data), replace = True))\n",
    "    oob_indices = [i for i in range(len(data)) if i not in bootstrap_indices]\n",
    "    \n",
    "    data_bootstrap = data.iloc[bootstrap_indices]\n",
    "    \n",
    "    data_oob = data.iloc[oob_indices]\n",
    "    \n",
    "    return data_bootstrap, data_oob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Trees to Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_example(example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    # print(question)\n",
    "    feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "    # ask question\n",
    "    if comparison_operator == \"<=\":  # feature is continuous\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    \n",
    "    # feature is categorical\n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return classify_example(example, residual_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df, tree):\n",
    "    \n",
    "    df[\"classification\"] = df.apply(classify_example, axis=1, args=(tree,))\n",
    "    df[\"classification_correct\"] = df[\"classification\"] == df[\"label\"]\n",
    "    \n",
    "    accuracy = df[\"classification_correct\"].mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(data, n_estimators, max_features=5, max_depth=5, min_samples=2):\n",
    "    \n",
    "    tree_ls = list()\n",
    "    oob_ls = list()\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        data_boot, data_oob = draw_bootstrap(data)\n",
    "        tree = decision_tree_algorithm(data_boot, counter=0, min_samples=min_samples, max_depth=max_depth, max_features=max_features)\n",
    "        tree_ls.append(tree)\n",
    "        oob_error = 1 - calculate_accuracy(data_oob, tree)\n",
    "        oob_ls.append(oob_error)\n",
    "    \n",
    "    print(np.mean(oob_ls))\n",
    "    \n",
    "    return tree_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(val, trees):\n",
    "    \n",
    "    votes = []\n",
    "    \n",
    "    for tree in trees:\n",
    "        votes.append(classify_example(val, tree))\n",
    "    \n",
    "    counter = Counter(votes)\n",
    "    \n",
    "    return counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data, trees): \n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        result = predict(data.iloc[i], trees)\n",
    "        predictions.append(result)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, predictions, label='label'):\n",
    "    data['prediction'] = predictions \n",
    "    data['correct'] = data['prediction'] == data[label]\n",
    "    return data['correct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_testing(data, trees):\n",
    "    predictions = prediction(data, trees)\n",
    "    acc = evaluate(data, predictions)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_indices(data, k):\n",
    "    fold_size = len(data) // k\n",
    "    indices = np.arange(len(data))\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        test_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        train_indices = np.concatenate([indices[:i * fold_size], indices[(i + 1) * fold_size:]])\n",
    "        folds.append((train_indices, test_indices))\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(data, k, estimators):\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    fold_indices = kfold_indices(data, k)\n",
    "    \n",
    "    for train_indices, test_indices in fold_indices:\n",
    "        data_train = data.iloc[train_indices]\n",
    "        data_test = data.iloc[test_indices]\n",
    "        \n",
    "        trees = random_forest(data_train, n_estimators=estimators)\n",
    "        fold_score = model_testing(data_test, trees)\n",
    "        \n",
    "        scores.append(fold_score)\n",
    "    \n",
    "    print(f'mean accuracy score: {np.mean(scores)}')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
